# Datasets / Benchmarks

Here are the public/open-source datasets or benchmarks used or most relevant to the research paper, formatted as a Markdown list:

*   **APIGen Dataset (Salesforce/xlam-function-calling-60k)**
    *   **Description:** A synthetic function-calling dataset containing 60,000 high-quality entries, generated by the APIGen pipeline, designed to advance the field of function-calling agent domains. It includes 3,673 executable APIs across 21 categories and various query styles, including parallel function calling data.
    *   **Direct Link:** [https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k](https://huggingface.co/datasets/Salesforce/xlam-function-calling-60k)

*   **APIGen Project Homepage**
    *   **Description:** The official project homepage for APIGen, where the dataset can also be viewed and downloaded, and additional information about the pipeline is provided.
    *   **Direct Link:** [https://apigen-pipeline.github.io/](https://apigen-pipeline.github.io/)

*   **Berkeley Function-Calling Leaderboard (BFCL)**
    *   **Description:** A robust and comprehensive framework providing 2,000 test cases for evaluating models' abilities to call functions across a wide range of scenarios and programming languages. It serves as a benchmark for assessing function-calling capabilities of LLMs.
    *   **Direct Link:** [https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html)

*   **ToolBench**
    *   **Description:** A comprehensive tool-use dataset that includes over 16,000 REST APIs across 49 coarse-grained categories from RapidAPI Hub. It is an instruction-tuning dataset for tool use. (The paper used APIs sourced from ToolBench).
    *   **Direct Link:** Not directly provided in the paper, but referenced as [10] (ToolLLM project).

*   **APIBank**
    *   **Description:** A benchmark designed for tool-augmented Large Language Models (LLMs), providing a training set that contains tool-use dialogues from various APIs.
    *   **Direct Link:** Not directly provided in the paper, but referenced as [12].

*   **AgentInstruct**
    *   **Description:** An umbrella dataset that consists of 6 sub-datasets designed for various agent tasks. The paper mentions it includes:
        *   **AlfWorld:** Focused on aligning text and embodied environments for interactive learning.
        *   **WebShop:** Focused on scalable real-world web interaction with grounded language agents.
        *   **Mind2Web:** Aimed at developing a generalist agent for the web.
        *   **Knowledge Graph:** A category within AgentInstruct for agent tasks involving knowledge graphs.
        *   **Operating System:** A category within AgentInstruct for agent tasks involving operating systems.
        *   **Database:** A category within AgentInstruct for agent tasks involving databases.
    *   **Direct Link:** Not directly provided in the paper, but referenced as [18] (for AgentInstruct) and [26], [27], [28] for its sub-components.

*   **Toolalpaca**
    *   **Description:** A varied and well-structured tool-use dataset constructed by randomly selecting APIs and generating documentation.
    *   **Direct Link:** Not directly provided in the paper, but referenced as [30].

*   **AgentBoard**
    *   **Description:** A benchmark that includes 9 tasks, with specific tasks like ToolOperation and ToolQuery designed to evaluate an agent's ability on multi-turn interaction with external tools.
    *   **Direct Link:** Not directly provided in the paper, but referenced as [34].

*   **Stable Toolbench server**
    *   **Description:** A server used for API accessibility testing by the authors, featuring a vast array of developer-contributed APIs. (It is a *server* providing APIs rather than a static dataset file).
    *   **Direct Link:** Not directly provided in the paper, but referenced as [38].